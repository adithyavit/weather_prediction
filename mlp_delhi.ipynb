{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 267,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load in "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "from sklearn.neural_network import MLPClassifier as M\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "sdd = pd.read_csv('/Users/adithya/Downloads/machine_learning/ML_LAB/clean_Data_delhi3.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "metadata": {},
   "outputs": [],
   "source": [
    "train=pd.read_csv('/Users/adithya/Downloads/machine_learning/ML_LAB/delhi_train.csv')\n",
    "test=pd.read_csv('/Users/adithya/Downloads/machine_learning/ML_LAB/test_delhi.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "metadata": {},
   "outputs": [],
   "source": [
    "y=train['conds']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "metadata": {},
   "outputs": [],
   "source": [
    "le = LabelEncoder().fit(train.conds) \n",
    "labels = le.transform(train.conds)           # encode species strings\n",
    "classes = list(le.classes_)                    # save column names for submission\n",
    "test_ids = test.id                             # save test ids for submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, ..., 1, 1, 1])"
      ]
     },
     "execution_count": 275,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = train.drop(['conds','id'], axis=1)\n",
    "test = test.drop(['id'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "MLP=M(hidden_layer_sizes=(192,512,128),random_state=42,solver='adam',tol=0.00001,activation='tanh',learning_rate='adaptive',verbose=10)\n",
    "Scale=StandardScaler()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {},
   "outputs": [],
   "source": [
    "T=Scale.fit_transform(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 0.02289329\n",
      "Iteration 2, loss = 0.01600899\n",
      "Iteration 3, loss = 0.01348388\n",
      "Iteration 4, loss = 0.01287536\n",
      "Iteration 5, loss = 0.01228774\n",
      "Iteration 6, loss = 0.01135343\n",
      "Iteration 7, loss = 0.01035292\n",
      "Iteration 8, loss = 0.01013642\n",
      "Iteration 9, loss = 0.01039376\n",
      "Iteration 10, loss = 0.00940667\n",
      "Iteration 11, loss = 0.00932836\n",
      "Iteration 12, loss = 0.00947757\n",
      "Iteration 13, loss = 0.00935950\n",
      "Iteration 14, loss = 0.00847284\n",
      "Iteration 15, loss = 0.00850648\n",
      "Iteration 16, loss = 0.00842711\n",
      "Iteration 17, loss = 0.00867237\n",
      "Iteration 18, loss = 0.00853598\n",
      "Iteration 19, loss = 0.00778391\n",
      "Iteration 20, loss = 0.00737301\n",
      "Iteration 21, loss = 0.00800919\n",
      "Iteration 22, loss = 0.00800771\n",
      "Iteration 23, loss = 0.00792717\n",
      "Training loss did not improve more than tol=0.000010 for two consecutive epochs. Stopping.\n",
      "time taken for tanh in 3 layers using 192,512,128 neurons 101.45115280151367 seconds ---\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "start_time = time.time()\n",
    "MLP.fit(T,labels)\n",
    "print(\"time taken for tanh in 3 layers using 192,512,128 neurons %s seconds ---\" % (time.time() - start_time))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.99739235647530045"
      ]
     },
     "execution_count": 212,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MLP.score(T, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "T=Scale.fit_transform(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "MLP=M(hidden_layer_sizes=(192,512,128),random_state=42,solver='adam',tol=0.00001,activation='logistic',learning_rate='adaptive',verbose=10)\n",
    "Scale=StandardScaler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 0.02289329\n",
      "Iteration 2, loss = 0.01600899\n",
      "Iteration 3, loss = 0.01348388\n",
      "Iteration 4, loss = 0.01287536\n",
      "Iteration 5, loss = 0.01228774\n",
      "Iteration 6, loss = 0.01135343\n",
      "Iteration 7, loss = 0.01035292\n",
      "Iteration 8, loss = 0.01013642\n",
      "Iteration 9, loss = 0.01039376\n",
      "Iteration 10, loss = 0.00940667\n",
      "Iteration 11, loss = 0.00932836\n",
      "Iteration 12, loss = 0.00947757\n",
      "Iteration 13, loss = 0.00935950\n",
      "Iteration 14, loss = 0.00847284\n",
      "Iteration 15, loss = 0.00850648\n",
      "Iteration 16, loss = 0.00842711\n",
      "Iteration 17, loss = 0.00867237\n",
      "Iteration 18, loss = 0.00853598\n",
      "Iteration 19, loss = 0.00778391\n",
      "Iteration 20, loss = 0.00737301\n",
      "Iteration 21, loss = 0.00800919\n",
      "Iteration 22, loss = 0.00800771\n",
      "Iteration 23, loss = 0.00792717\n",
      "Training loss did not improve more than tol=0.000010 for two consecutive epochs. Stopping.\n",
      "time taken for logistic in 3 layers using 192,512,128 neurons 94.9527940750122 seconds ---\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "start_time = time.time()\n",
    "MLP.fit(T,labels)\n",
    "print(\"time taken for logistic in 3 layers using 192,512,128 neurons %s seconds ---\" % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.99528538050734316"
      ]
     },
     "execution_count": 216,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MLP.score(T, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "T=Scale.fit_transform(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "MLP=M(hidden_layer_sizes=(192,512,128),random_state=42,solver='adam',tol=0.00001,activation='identity',learning_rate='adaptive',verbose=10)\n",
    "Scale=StandardScaler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 0.02289329\n",
      "Iteration 2, loss = 0.01600899\n",
      "Iteration 3, loss = 0.01348388\n",
      "Iteration 4, loss = 0.01287536\n",
      "Iteration 5, loss = 0.01228774\n",
      "Iteration 6, loss = 0.01135343\n",
      "Iteration 7, loss = 0.01035292\n",
      "Iteration 8, loss = 0.01013642\n",
      "Iteration 9, loss = 0.01039376\n",
      "Iteration 10, loss = 0.00940667\n",
      "Iteration 11, loss = 0.00932836\n",
      "Iteration 12, loss = 0.00947757\n",
      "Iteration 13, loss = 0.00935950\n",
      "Iteration 14, loss = 0.00847284\n",
      "Iteration 15, loss = 0.00850648\n",
      "Iteration 16, loss = 0.00842711\n",
      "Iteration 17, loss = 0.00867237\n",
      "Iteration 18, loss = 0.00853598\n",
      "Iteration 19, loss = 0.00778391\n",
      "Iteration 20, loss = 0.00737301\n",
      "Iteration 21, loss = 0.00800919\n",
      "Iteration 22, loss = 0.00800771\n",
      "Iteration 23, loss = 0.00792717\n",
      "Training loss did not improve more than tol=0.000010 for two consecutive epochs. Stopping.\n",
      "time taken for identity in 3 layers using 192,512,128 neurons 89.9529321193695 seconds ---\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "start_time = time.time()\n",
    "MLP.fit(T,labels)\n",
    "print(\"time taken for identity in 3 layers using 192,512,128 neurons %s seconds ---\" % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.99461782376502006"
      ]
     },
     "execution_count": 221,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MLP.score(T, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "T=Scale.fit_transform(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "MLP=M(hidden_layer_sizes=(192,512,128),random_state=42,solver='adam',tol=0.00001,activation='relu',learning_rate='adaptive',verbose=10)\n",
    "Scale=StandardScaler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 0.02289329\n",
      "Iteration 2, loss = 0.01600899\n",
      "Iteration 3, loss = 0.01348388\n",
      "Iteration 4, loss = 0.01287536\n",
      "Iteration 5, loss = 0.01228774\n",
      "Iteration 6, loss = 0.01135343\n",
      "Iteration 7, loss = 0.01035292\n",
      "Iteration 8, loss = 0.01013642\n",
      "Iteration 9, loss = 0.01039376\n",
      "Iteration 10, loss = 0.00940667\n",
      "Iteration 11, loss = 0.00932836\n",
      "Iteration 12, loss = 0.00947757\n",
      "Iteration 13, loss = 0.00935950\n",
      "Iteration 14, loss = 0.00847284\n",
      "Iteration 15, loss = 0.00850648\n",
      "Iteration 16, loss = 0.00842711\n",
      "Iteration 17, loss = 0.00867237\n",
      "Iteration 18, loss = 0.00853598\n",
      "Iteration 19, loss = 0.00778391\n",
      "Iteration 20, loss = 0.00737301\n",
      "Iteration 21, loss = 0.00800919\n",
      "Iteration 22, loss = 0.00800771\n",
      "Iteration 23, loss = 0.00792717\n",
      "Training loss did not improve more than tol=0.000010 for two consecutive epochs. Stopping.\n",
      "time taken for relu in 3 layers using 192,512,128 neurons 98.4579598903656 seconds ---\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "start_time = time.time()\n",
    "MLP.fit(T,labels)\n",
    "print(\"time taken for relu in 3 layers using 192,512,128 neurons %s seconds ---\" % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.99747580106809075"
      ]
     },
     "execution_count": 225,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MLP.score(T, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "predict=MLP.predict_proba(Scale.transform(test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "MLP=M(hidden_layer_sizes=(192),random_state=42,solver='adam',tol=0.00001,activation='tanh',learning_rate='adaptive',verbose=10)\n",
    "Scale=StandardScaler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "T=Scale.fit_transform(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 0.12496596\n",
      "Iteration 2, loss = 0.02719212\n",
      "Iteration 3, loss = 0.01919030\n",
      "Iteration 4, loss = 0.01646108\n",
      "Iteration 5, loss = 0.01513744\n",
      "Iteration 6, loss = 0.01466769\n",
      "Iteration 7, loss = 0.01434747\n",
      "Iteration 8, loss = 0.01411833\n",
      "Iteration 9, loss = 0.01389949\n",
      "Iteration 10, loss = 0.01375796\n",
      "Iteration 11, loss = 0.01361022\n",
      "Iteration 12, loss = 0.01344120\n",
      "Iteration 13, loss = 0.01332369\n",
      "Iteration 14, loss = 0.01324272\n",
      "Iteration 15, loss = 0.01295961\n",
      "Iteration 16, loss = 0.01302581\n",
      "Iteration 17, loss = 0.01277279\n",
      "Iteration 18, loss = 0.01265082\n",
      "Iteration 19, loss = 0.01272827\n",
      "Iteration 20, loss = 0.01239042\n",
      "Iteration 21, loss = 0.01240706\n",
      "Iteration 22, loss = 0.01209502\n",
      "Iteration 23, loss = 0.01211564\n",
      "Iteration 24, loss = 0.01204733\n",
      "Iteration 25, loss = 0.01196778\n",
      "Iteration 26, loss = 0.01176254\n",
      "Iteration 27, loss = 0.01164523\n",
      "Iteration 28, loss = 0.01160791\n",
      "Iteration 29, loss = 0.01155041\n",
      "Iteration 30, loss = 0.01134393\n",
      "Iteration 31, loss = 0.01131146\n",
      "Iteration 32, loss = 0.01115258\n",
      "Iteration 33, loss = 0.01114647\n",
      "Iteration 34, loss = 0.01132320\n",
      "Iteration 35, loss = 0.01098212\n",
      "Iteration 36, loss = 0.01092560\n",
      "Iteration 37, loss = 0.01079441\n",
      "Iteration 38, loss = 0.01072999\n",
      "Iteration 39, loss = 0.01057621\n",
      "Iteration 40, loss = 0.01053154\n",
      "Iteration 41, loss = 0.01040163\n",
      "Iteration 42, loss = 0.01031496\n",
      "Iteration 43, loss = 0.01049370\n",
      "Iteration 44, loss = 0.01033316\n",
      "Iteration 45, loss = 0.01020354\n",
      "Iteration 46, loss = 0.01011031\n",
      "Iteration 47, loss = 0.01000526\n",
      "Iteration 48, loss = 0.01005014\n",
      "Iteration 49, loss = 0.00983163\n",
      "Iteration 50, loss = 0.00977416\n",
      "Iteration 51, loss = 0.00967526\n",
      "Iteration 52, loss = 0.00958716\n",
      "Iteration 53, loss = 0.00967714\n",
      "Iteration 54, loss = 0.00949805\n",
      "Iteration 55, loss = 0.00953365\n",
      "Iteration 56, loss = 0.00955020\n",
      "Iteration 57, loss = 0.00929299\n",
      "Iteration 58, loss = 0.00929032\n",
      "Iteration 59, loss = 0.00934395\n",
      "Iteration 60, loss = 0.00919422\n",
      "Iteration 61, loss = 0.00905794\n",
      "Iteration 62, loss = 0.00921840\n",
      "Iteration 63, loss = 0.00907470\n",
      "Iteration 64, loss = 0.00902950\n",
      "Iteration 65, loss = 0.00892280\n",
      "Iteration 66, loss = 0.00894019\n",
      "Iteration 67, loss = 0.00879952\n",
      "Iteration 68, loss = 0.00888581\n",
      "Iteration 69, loss = 0.00881446\n",
      "Iteration 70, loss = 0.00883625\n",
      "Training loss did not improve more than tol=0.000010 for two consecutive epochs. Stopping.\n",
      "time taken for tanh in 1 layer using 192 neurons 22.609812021255493 seconds ---\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "start_time = time.time()\n",
    "MLP.fit(T,labels)\n",
    "print(\"time taken for tanh in 1 layer using 192 neurons %s seconds ---\" % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.99695427236315082"
      ]
     },
     "execution_count": 238,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MLP.score(T, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "MLP=M(hidden_layer_sizes=(192),random_state=42,solver='adam',tol=0.00001,activation='identity',learning_rate='adaptive',verbose=10)\n",
    "Scale=StandardScaler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "T=Scale.fit_transform(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 0.12433495\n",
      "Iteration 2, loss = 0.03008147\n",
      "Iteration 3, loss = 0.02159816\n",
      "Iteration 4, loss = 0.01794690\n",
      "Iteration 5, loss = 0.01626370\n",
      "Iteration 6, loss = 0.01562894\n",
      "Iteration 7, loss = 0.01526524\n",
      "Iteration 8, loss = 0.01505491\n",
      "Iteration 9, loss = 0.01491651\n",
      "Iteration 10, loss = 0.01484290\n",
      "Iteration 11, loss = 0.01475876\n",
      "Iteration 12, loss = 0.01479006\n",
      "Iteration 13, loss = 0.01476492\n",
      "Iteration 14, loss = 0.01475928\n",
      "Training loss did not improve more than tol=0.000010 for two consecutive epochs. Stopping.\n",
      "time taken for identity in 1 layer using 192 neurons 2.7491960525512695 seconds ---\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "start_time = time.time()\n",
    "MLP.fit(T,labels)\n",
    "print(\"time taken for identity in 1 layer using 192 neurons %s seconds ---\" % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.99493074098798395"
      ]
     },
     "execution_count": 247,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MLP.score(T, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "MLP=M(hidden_layer_sizes=(192),random_state=42,solver='adam',tol=0.00001,activation='logistic',learning_rate='adaptive',verbose=10)\n",
    "Scale=StandardScaler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "T=Scale.fit_transform(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 0.13296503\n",
      "Iteration 2, loss = 0.05572465\n",
      "Iteration 3, loss = 0.03617734\n",
      "Iteration 4, loss = 0.02758712\n",
      "Iteration 5, loss = 0.02309443\n",
      "Iteration 6, loss = 0.02045403\n",
      "Iteration 7, loss = 0.01871836\n",
      "Iteration 8, loss = 0.01748426\n",
      "Iteration 9, loss = 0.01664210\n",
      "Iteration 10, loss = 0.01602047\n",
      "Iteration 11, loss = 0.01555712\n",
      "Iteration 12, loss = 0.01526115\n",
      "Iteration 13, loss = 0.01496968\n",
      "Iteration 14, loss = 0.01483837\n",
      "Iteration 15, loss = 0.01466110\n",
      "Iteration 16, loss = 0.01465316\n",
      "Iteration 17, loss = 0.01448541\n",
      "Iteration 18, loss = 0.01443798\n",
      "Iteration 19, loss = 0.01447297\n",
      "Iteration 20, loss = 0.01435097\n",
      "Iteration 21, loss = 0.01442676\n",
      "Iteration 22, loss = 0.01433088\n",
      "Iteration 23, loss = 0.01432270\n",
      "Iteration 24, loss = 0.01432305\n",
      "Iteration 25, loss = 0.01432646\n",
      "Training loss did not improve more than tol=0.000010 for two consecutive epochs. Stopping.\n",
      "time taken for logistic in 1 layer using 192 neurons 8.095012903213501 seconds ---\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "start_time = time.time()\n",
    "MLP.fit(T,labels)\n",
    "print(\"time taken for logistic in 1 layer using 192 neurons %s seconds ---\" % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.99541054739652868"
      ]
     },
     "execution_count": 254,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MLP.score(T, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "MLP=M(hidden_layer_sizes=(192),random_state=42,solver='adam',tol=0.00001,activation='relu',learning_rate='adaptive',verbose=10)\n",
    "Scale=StandardScaler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "T=Scale.fit_transform(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 0.14160618\n",
      "Iteration 2, loss = 0.02737875\n",
      "Iteration 3, loss = 0.01983146\n",
      "Iteration 4, loss = 0.01671411\n",
      "Iteration 5, loss = 0.01528987\n",
      "Iteration 6, loss = 0.01465748\n",
      "Iteration 7, loss = 0.01405628\n",
      "Iteration 8, loss = 0.01374552\n",
      "Iteration 9, loss = 0.01339927\n",
      "Iteration 10, loss = 0.01312003\n",
      "Iteration 11, loss = 0.01298811\n",
      "Iteration 12, loss = 0.01267712\n",
      "Iteration 13, loss = 0.01261997\n",
      "Iteration 14, loss = 0.01253332\n",
      "Iteration 15, loss = 0.01225680\n",
      "Iteration 16, loss = 0.01224729\n",
      "Iteration 17, loss = 0.01203604\n",
      "Iteration 18, loss = 0.01190396\n",
      "Iteration 19, loss = 0.01193055\n",
      "Iteration 20, loss = 0.01165110\n",
      "Iteration 21, loss = 0.01166693\n",
      "Iteration 22, loss = 0.01133763\n",
      "Iteration 23, loss = 0.01131012\n",
      "Iteration 24, loss = 0.01122029\n",
      "Iteration 25, loss = 0.01111963\n",
      "Iteration 26, loss = 0.01091738\n",
      "Iteration 27, loss = 0.01080020\n",
      "Iteration 28, loss = 0.01076414\n",
      "Iteration 29, loss = 0.01070942\n",
      "Iteration 30, loss = 0.01041717\n",
      "Iteration 31, loss = 0.01042000\n",
      "Iteration 32, loss = 0.01023020\n",
      "Iteration 33, loss = 0.01026249\n",
      "Iteration 34, loss = 0.01036773\n",
      "Iteration 35, loss = 0.01010509\n",
      "Iteration 36, loss = 0.01004164\n",
      "Iteration 37, loss = 0.00992636\n",
      "Iteration 38, loss = 0.00988059\n",
      "Iteration 39, loss = 0.00971993\n",
      "Iteration 40, loss = 0.00969357\n",
      "Iteration 41, loss = 0.00961467\n",
      "Iteration 42, loss = 0.00951904\n",
      "Iteration 43, loss = 0.00973219\n",
      "Iteration 44, loss = 0.00955559\n",
      "Iteration 45, loss = 0.00947042\n",
      "Iteration 46, loss = 0.00939271\n",
      "Iteration 47, loss = 0.00924225\n",
      "Iteration 48, loss = 0.00934960\n",
      "Iteration 49, loss = 0.00914334\n",
      "Iteration 50, loss = 0.00908514\n",
      "Iteration 51, loss = 0.00906202\n",
      "Iteration 52, loss = 0.00899651\n",
      "Iteration 53, loss = 0.00906655\n",
      "Iteration 54, loss = 0.00894307\n",
      "Iteration 55, loss = 0.00894357\n",
      "Iteration 56, loss = 0.00895110\n",
      "Iteration 57, loss = 0.00869105\n",
      "Iteration 58, loss = 0.00869801\n",
      "Iteration 59, loss = 0.00870904\n",
      "Iteration 60, loss = 0.00858072\n",
      "Iteration 61, loss = 0.00844777\n",
      "Iteration 62, loss = 0.00859459\n",
      "Iteration 63, loss = 0.00840568\n",
      "Iteration 64, loss = 0.00839484\n",
      "Iteration 65, loss = 0.00830739\n",
      "Iteration 66, loss = 0.00878590\n",
      "Iteration 67, loss = 0.00810783\n",
      "Iteration 68, loss = 0.00821595\n",
      "Iteration 69, loss = 0.00812806\n",
      "Iteration 70, loss = 0.00816524\n",
      "Training loss did not improve more than tol=0.000010 for two consecutive epochs. Stopping.\n",
      "time taken for relu in 1 layer using 192 neurons 21.071067094802856 seconds ---\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "start_time = time.time()\n",
    "MLP.fit(T,labels)\n",
    "print(\"time taken for relu in 1 layer using 192 neurons %s seconds ---\" % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.99732977303070758"
      ]
     },
     "execution_count": 261,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MLP.score(T, labels)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
